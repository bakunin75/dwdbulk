- Add filter to forecast import (by station & parameter)
- Setup parquet files (with partitioning for forecast import)
- Add tests to forecast import
- Add forecast import to orchestrate.py
- Ensure file cleanup (erase directories)

- Update orchestrate.py so that only berlin (and brandenburg?) data is imported
- Fix parquet files so that partitioning happens on initial file write (remove second partitioning step)

- Setup ongoing data collection via Azure pipelines (hourly?)

- Start on blogpost about library
- Work on couple of viz (with forecasts & how change over time...)

- Add badges to readme
- Move to cloud (Azure?)
- https://pypi.org/project/azure-storage-blob/
- Process data files -> parquet with partitioning
    - Add to prefect flow
- Put together exploratory visualizations
    - Blog post about sleeping weather
    - Blog post about heat & duration
    - Blog post about cloudiness (overall vs daytime)


- Add Metadata import..
- Change "height" field to "elevation"
- Identify and correct timezones! (Starting in 2000 UTC, up til 1999 in CET);
    - For correct parsing, metadata Needs to be extracted

- Add file check step for parquet files; delete corrupted files...

- Log processed files (to avoid reprocessing!) (check update timestamps to avoid special case for 'latest' files)
- Cleanup directory after downloading files...
